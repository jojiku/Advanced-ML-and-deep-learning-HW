{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQp47ajD2Ga4"
   },
   "source": [
    "# Знакомство с word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Pat15ls9N8"
   },
   "source": [
    "## Загрузка модели\n",
    "Скачаем модель <code>google-news-vectors</code>. Откроем ее с помощью библиотеки <code>gensim</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nd-xNyAGy1tT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tseh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tseh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tseh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tseh\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install -q -U gensim\n",
    "! pip install -q SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-4xfcycMynhZ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    " \n",
    "w = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", \n",
    "                                      binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6JtQjaORfzA"
   },
   "source": [
    "Структура называется <code>KeyedVectors</code> и по сути представляет собой отображение между ключами и векторами. Каждый вектор идентифицируется своим ключом поиска, чаще всего коротким строковым токеном, поэтому обычно это соответствие между\n",
    "\n",
    "<center><code>{str => 1D numpy array}</code></center><br/>\n",
    "\n",
    "\n",
    "\n",
    "Например, выведем первые 10 координат вектора, соответствующего слову <code>sunrise</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ol9DuE6VRfzH",
    "outputId": "e21572c1-90c1-44d8-f2c4-6bc6418ebc67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность вектора:  (300,)\n",
      "Первые 10 координат вектора: \n",
      " [-0.22558594 -0.03540039 -0.21679688  0.03613281 -0.2265625  -0.09814453\n",
      "  0.109375   -0.34570312  0.18652344  0.01806641]\n"
     ]
    }
   ],
   "source": [
    "print(\"Размерность вектора: \", w[\"sunrise\"].shape)\n",
    "print(\"Первые 10 координат вектора: \\n\", w[\"sunrise\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rv9Rqvq2af8"
   },
   "source": [
    "## Задание 1. Сходство. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mns8gpJFRfzd"
   },
   "source": [
    "Извлеките векторы слов <code>London</code>, <code>England</code>, <code>Moscow</code>. Посчитайте косинусное расстояние между словами <code>London</code> и <code>England</code> и между словами <code>Moscow</code> и <code>England</code>. Какая пара слов ближе? Подсказка: для вычисления косинусного расстояния использвется метод <code>distance()</code>. Правильный ответ представлен в блоке вывода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5600714385509491\n",
      "0.8476868271827698\n"
     ]
    }
   ],
   "source": [
    "print(w.distance(\"London\", \"England\"))\n",
    "print(w.distance(\"Moscow\", \"England\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLXEcSxt3DG4"
   },
   "source": [
    "## Задание 2. Аналогии.\n",
    "С помощью метода most_similar решите аналогию\n",
    "```London : England = Moscow : X```\n",
    "\n",
    "Правильный ответ представлен в блоке вывода.\n",
    "\n",
    "(Подсказка: нужно использовать аргументы positive и negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Russia', 0.6502718329429626),\n",
       " ('Ukraine', 0.5879061818122864),\n",
       " ('Belarus', 0.5666376352310181),\n",
       " ('Azerbaijan', 0.5418694615364075),\n",
       " ('Armenia', 0.5300518870353699),\n",
       " ('Poland', 0.5253247618675232),\n",
       " ('coach_Georgy_Yartsev', 0.5220180749893188),\n",
       " ('Russian', 0.5214669108390808),\n",
       " ('Croatia', 0.5166040658950806),\n",
       " ('Moldova', 0.5125792026519775)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_result = w.most_similar(positive=['Moscow', 'England'], negative=['London'])\n",
    "analogy_result[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFzneqrn3Djq"
   },
   "source": [
    "## Задание 3. Сходство: найти лишнее. \n",
    "С помощью метода <code>doesnt_match</code> найдите лишнее слово в ряду <code>breakfast cereal dinner lunch</code>.\n",
    "\n",
    "Правильный ответ представлен в блоке вывода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT-Zl3YaRf0X"
   },
   "source": [
    "## Задание 4. Представление предложений в виде векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dm_SiyjU3D9G"
   },
   "source": [
    "\n",
    "Дано предложение: <code>the quick brown fox jumps over the lazy dog</code>. Вам нужно представить это предложение в виде вектора. Для этого найдите вектор каждого слова в модели, а затем усредните векторы покомпонентно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор предложения: [0.09055582682291667, 0.054341634114583336, -0.067138671875, 0.10968695746527778, -0.010606553819444444]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "words = sentence.split()\n",
    "\n",
    "sum_vector = [0] * 300 \n",
    "\n",
    "for word in words:\n",
    "    if word in w.key_to_index: \n",
    "        word_vector = w[word]\n",
    "        sum_vector = [sum(x) for x in zip(sum_vector, word_vector)]\n",
    "\n",
    "average_vector = [x / len(words) for x in sum_vector]\n",
    "\n",
    "print(\"Вектор предложения:\", average_vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3hwN53r5un"
   },
   "source": [
    "# Сравнение двух моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-HvrEkHtFqQ"
   },
   "source": [
    "## Загрузка ещё одной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z13Io-4x3Ve2"
   },
   "source": [
    "\n",
    "Откроем модель google-news-vectors и модель, обученную на британском национальном корпусе http://vectors.nlpl.eu/repository/20/0.zip, с помощью gensim. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-21wScRRf1E"
   },
   "source": [
    "Загрузим модель, обученную на британском национальном корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-E6OAvhw8-A7"
   },
   "outputs": [],
   "source": [
    "w_british = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GpisLDDRf1T"
   },
   "source": [
    "Заметим, что размерность векторов в этом случае также равна 300. При этом через нижнее подчеркивание нужно указывать часть речи используемого слова. Слова следует приводить к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "a7VEcvPIRf1W",
    "outputId": "dbed96cb-c7cc-44c1-f369-dfbfeb5096d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "lower is ok\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(w_british[\"London_NOUN\"].shape)\n",
    "    print('upper is ok')\n",
    "except:\n",
    "    print(w_british[\"london_NOUN\"].shape)\n",
    "    print('lower is ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfpohw153YQs"
   },
   "source": [
    "## Набор данных для оценки качества\n",
    "Скачаем датасет wordsim353. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6c2--gQ3bJF",
    "outputId": "45365012-20d3-4030-960a-5f0a3a1d5b82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "x wordsim353_sim_rel/wordsim353_agreed.txt\n",
      "x wordsim353_sim_rel/wordsim353_annotator1.txt\n",
      "x wordsim353_sim_rel/wordsim353_annotator2.txt\n",
      "x wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\n",
      "x wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\n",
      "\"head\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "! wget -c http://alfonseca.org/pubs/ws353simrel.tar.gz \n",
    "! tar -xvf ws353simrel.tar.gz\n",
    "! head -5 wordsim353_sim_rel/wordsim_similarity_goldstandard.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgCXUELHRf2E"
   },
   "source": [
    "## Подготовка эталонной выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqy84Dmp3bYa"
   },
   "source": [
    "\n",
    "Из файла `wordsim_similarity_goldstandard.txt` извлечем пары слов и посчитаем косинусное сходство их векторов в обеих моделях. Посчитаем корреляцию оценок сходства в модели google-news-vectors с оценками аннотаторов в датасете, а затем - корреляцию сходства в модели на основе британского национального корпуса с оценками аннотаторов в датасете. Какая модель ближе к суждениям экспертов-разметчиков?\n",
    "\n",
    "(используем только те слова из wordsim, для которых находятся векторы на британском корпусе, помеченные как существительные!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "Bpeg6FQd3clf",
    "outputId": "e83af633-273d-4e3a-a953-e9eac31c4350"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first second  score\n",
       "0  tiger    cat   7.35\n",
       "1  tiger  tiger  10.00\n",
       "2  plane    car   5.77"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"wordsim_similarity_goldstandard.txt\", \n",
    "                 sep=\"\\t\", header=None)\n",
    "df.columns = [\"first\", \"second\", \"score\"]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDcXFGZnRf2e",
    "tags": []
   },
   "source": [
    "## Вычисление оценок similarity моделей\n",
    "Используем только те слова из wordsim, для которых находятся векторы на британском корпусе, помеченные как существительные, сформируйте 3 массива с оценкам схожести: \n",
    "\n",
    "1. Оценки (косинус между векторами), полученные в результате модели google-news-vectors\n",
    "\n",
    "2. Оценки (косинус между векторами) полученные в результате модели на основе британского национального корпуса\n",
    "\n",
    "3. Эталонные оценки из word_sim, для слов из которых находятся векторы на британском корпусе\n",
    "\n",
    "Пропущенные слова из word_sim представлены в блоке вывода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stupid\n",
      "arafat\n",
      "harvard\n",
      "mexico\n",
      "live\n",
      "seven\n",
      "five\n",
      "mars\n"
     ]
    }
   ],
   "source": [
    "gn_dist, br_dist, scores = [], [], []\n",
    "def add_stuff(word):\n",
    "    return word + '_NOUN'\n",
    "\n",
    "for row in df.iterrows():\n",
    "    \n",
    "  w1, w2 = row[1][\"first\"].lower(), row[1][\"second\"].lower()\n",
    "    \n",
    "  try:\n",
    "        if add_stuff(w1) in w_british.key_to_index and add_stuff(w2) in w_british.key_to_index:\n",
    "            gn_dist.append(w.similarity(w1, w2))\n",
    "            br_dist.append(w_british.similarity(add_stuff(w1), add_stuff(w2)))\n",
    "            scores.append(row[1][\"score\"])\n",
    "            \n",
    "        elif add_stuff(w1) not in w_british.key_to_index:\n",
    "            print(w1)\n",
    "        elif add_stuff(w2) not in w_british.key_to_index:\n",
    "            print(w2)\n",
    "    \n",
    "  except KeyError as e:\n",
    "    print(e, \"Skipping this word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPyeIR2QtSec"
   },
   "source": [
    "## Выбор модели: корреляция с экспертами\n",
    "\n",
    "Вычислите корреляцию Спирмена для каждой модели по сравнению с эталонными оценками из word_sim.\n",
    "\n",
    "Результаты представлены в блоке вывода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Корреляция Спирмена для модели 'google-news-vectors': 0.7842376265134542\n",
      "Корреляция Спирмена для модели 'британский корпус': 0.7644278401221205\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearman_corr_google = spearmanr(gn_dist, scores)\n",
    "spearman_corr_british = spearmanr(br_dist, scores)\n",
    "print(\"Корреляция Спирмена для модели 'google-news-vectors':\", spearman_corr_google.correlation)\n",
    "print(\"Корреляция Спирмена для модели 'британский корпус':\", spearman_corr_british.correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZlbnwcq-SCL",
    "outputId": "698399f9-761b-4fac-a238-20fb77606d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GN spearmanr corr: 0.7817164245392593\n",
      "British spearmanr corr: 0.7627551934489611\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "#enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtlAncsQANfx"
   },
   "source": [
    "Можно заметить, что модель google-news-vectors несколько выигрывает в данном случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индивидуальное задание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Model Distance: 0.839\n",
      "British Model Distance: 0.886\n"
     ]
    }
   ],
   "source": [
    "distance_google = w.distance('media', 'bread')\n",
    "distance_british = w_british.distance('media_NOUN', 'bread_NOUN')\n",
    "\n",
    "print(f\"Google Model Distance: {distance_google:.3f}\")\n",
    "print(f\"British Model Distance: {distance_british:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Model Distance: media\n",
      "British Model Distance: media_NOUN\n"
     ]
    }
   ],
   "source": [
    "doesnt_match_g = w.doesnt_match(['media', 'bread', 'cucumber', 'doctor'])\n",
    "doesnt_match_b = w_british.doesnt_match(['media_NOUN', 'bread_NOUN', 'cucumber_NOUN', 'doctor_NOUN'])\n",
    "\n",
    "print(\"Google Model Distance:\", doesnt_match_g)\n",
    "print(\"British Model Distance:\", doesnt_match_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'chain is only as strong as its weakest link'\n",
    "s2 = 'Actions speak louder than words'\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    sum_vector = [0] * 300 \n",
    "    for word in words:\n",
    "        if word in w.key_to_index: \n",
    "            word_vector = w[word]\n",
    "            sum_vector = [sum(x) for x in zip(sum_vector, word_vector)]\n",
    "    average_vector = [x / len(words) for x in sum_vector]\n",
    "    return average_vector\n",
    "vec1 = vectorize(s1)\n",
    "vec2 = vectorize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Косинусное расстояние между предложениями: 0.784\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "s1 = 'chain is only as strong as its weakest link'\n",
    "s2 = 'Actions speak louder than words'\n",
    "\n",
    "def vectorize(sentence, model):\n",
    "    words = sentence.split()\n",
    "    sum_vector = np.zeros(300)  # Создайте нулевой вектор с размерностью 300\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:\n",
    "            word_vector = model[word]\n",
    "            sum_vector = np.add(sum_vector, word_vector)\n",
    "    if len(words) > 0:\n",
    "        average_vector = np.divide(sum_vector, len(words))\n",
    "    else:\n",
    "        average_vector = sum_vector\n",
    "    return average_vector\n",
    "\n",
    "# Ваши векторы предложений\n",
    "vec1 = vectorize(s1, w)\n",
    "vec2 = vectorize(s2, w)\n",
    "\n",
    "# Вычислите косинусное расстояние между векторами\n",
    "cosine_distance = cosine(vec1, vec2)\n",
    "\n",
    "print(f\"Косинусное расстояние между предложениями: {cosine_distance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arafat\n",
      "harvard\n",
      "mexico\n"
     ]
    }
   ],
   "source": [
    "subsample = df.iloc[14:114]\n",
    "\n",
    "gn_dist, br_dist, scores = [], [], []\n",
    "def add_stuff(word):\n",
    "    return word + '_NOUN'\n",
    "\n",
    "for row in subsample.iterrows():\n",
    "    \n",
    "  w1, w2 = row[1][\"first\"].lower(), row[1][\"second\"].lower()\n",
    "    \n",
    "  try:\n",
    "        if add_stuff(w1) in w_british.key_to_index and add_stuff(w2) in w_british.key_to_index:\n",
    "            gn_dist.append(w.similarity(w1, w2))\n",
    "            br_dist.append(w_british.similarity(add_stuff(w1), add_stuff(w2)))\n",
    "            scores.append(row[1][\"score\"])\n",
    "            \n",
    "        elif add_stuff(w1) not in w_british.key_to_index:\n",
    "            print(w1)\n",
    "        elif add_stuff(w2) not in w_british.key_to_index:\n",
    "            print(w2)\n",
    "    \n",
    "  except KeyError as e:\n",
    "    print(e, \"Skipping this word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Корреляция Спирмена для модели 'google-news-vectors': 0.6859410413174328\n",
      "Корреляция Спирмена для модели 'британский корпус': 0.6460421691947137\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearman_corr_google = spearmanr(gn_dist, scores)\n",
    "spearman_corr_british = spearmanr(br_dist, scores)\n",
    "print(\"Корреляция Спирмена для модели 'google-news-vectors':\", spearman_corr_google.correlation)\n",
    "print(\"Корреляция Спирмена для модели 'британский корпус':\", spearman_corr_british.correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
